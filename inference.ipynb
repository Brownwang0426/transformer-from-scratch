{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 0: NVIDIA GeForce RTX 4090\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "max_length = 200\n",
        "\n",
        "sequence_size =  max_length             \n",
        "feature_size = 768          \n",
        "num_layers = 3                      \n",
        "num_heads = 4                \n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'tanh'\n",
        "initializer = \"xavier_normal\"\n",
        "optimizer = 'adam'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.000001       \n",
        "\n",
        "num_epochs = 100  \n",
        "batch_size = 1\n",
        "\n",
        "model_directory = f'model.pth'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# Initialize the model\n",
        "model = build_model(sequence_size,\n",
        "                    feature_size,\n",
        "                    num_layers,\n",
        "                    num_heads,\n",
        "                    hidden_activation,\n",
        "                    output_activation,\n",
        "                    initializer,\n",
        "                    optimizer,\n",
        "                    loss,\n",
        "                    bias,\n",
        "                    drop_rate,\n",
        "                    alpha)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# loading nn models\n",
        "model_dict = torch.load(model_directory)\n",
        "model.load_state_dict(model_dict[f'model'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " spoke\n",
            " spoke lime\n",
            " spoke lime lime\n",
            " spoke lime lime the\n",
            " spoke lime lime the lime\n",
            " spoke lime lime the lime the\n",
            " spoke lime lime the lime the the\n",
            " spoke lime lime the lime the the the\n",
            " spoke lime lime the lime the the the the\n",
            " spoke lime lime the lime the the the the the\n",
            " spoke lime lime the lime the the the the the the\n",
            " spoke lime lime the lime the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n",
            " spoke lime lime the lime the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 初始化 BERT tokenizer 和 vectorizer\n",
        "tokenizer  = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "vectorizer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 範例句子\n",
        "sentence = \"What is in front of the Notre Dame Main Buildin\"\n",
        "\n",
        "sentence = \"[CLS] \" + sentence + \" [SEP] \"\n",
        "\n",
        "response = ''\n",
        "\n",
        "for i in range(max_length):\n",
        "\n",
        "    # Step 1: Tokenize the sentences\n",
        "    tokenized_sentence = tokenizer(sentence, add_special_tokens=False, padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "    \n",
        "    # Step 2: Vectorize the sentences\n",
        "    input_id           = tokenized_sentence['input_ids']\n",
        "    attention_mask     = tokenized_sentence['attention_mask']\n",
        "    with torch.no_grad(): \n",
        "        input_vector   = vectorizer(input_id).last_hidden_state \n",
        "\n",
        "    input_vector = input_vector.to(device)\n",
        "    mask_2 = attention_mask[0].unsqueeze(1) * attention_mask[0].unsqueeze(0)\n",
        "    mask_2 = mask_2.unsqueeze(0).unsqueeze(0)\n",
        "    mask_2 = mask_2.to(device)\n",
        "    mask_1 = (mask_2 -1) * 1e20\n",
        "    mask_1 = mask_1.to(device)\n",
        "\n",
        "    model.eval()     \n",
        "    output                 = model(input_vector, (mask_1, mask_2))\n",
        "\n",
        "    vocab_embeddings       = vectorizer.get_input_embeddings().weight .to(device)\n",
        "    cos_sim                = F.cosine_similarity(output, vocab_embeddings, dim=1)\n",
        "    most_similar_token_idx = torch.argmax(cos_sim).item()\n",
        "\n",
        "    if most_similar_token_idx != 102:\n",
        "            \n",
        "        word = tokenizer.convert_ids_to_tokens(most_similar_token_idx)\n",
        "\n",
        "        sentence += ' ' + word\n",
        "\n",
        "        response += ' ' + word\n",
        "        print(response)\n",
        "\n",
        "    \n",
        "    \n",
        "    # vocab = tokenizer.get_vocab()\n",
        "    # while most_similar_token_idx in vocab and vocab[most_similar_token_idx].startswith('[unused'):\n",
        "    #     most_similar_token_idx = torch.argmax(cos_sim).item()  # Recompute the most similar token\n",
        "\n",
        "    # from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "    # tokenizer_ = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    # model_     = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    # output     = model_.generate(torch.tensor([[most_similar_token_idx]]), max_length=20)\n",
        "    # word       = tokenizer_.decode(output[0], skip_special_tokens=True)\n",
        "    # print(word)\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
