{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datasets import load_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Aj5V_vlwSxd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 0: NVIDIA GeForce RTX 4090\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the IMDB dataset\n",
        "dataset = load_dataset('squad')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q: To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?\n",
            "A: Saint Bernadette Soubirous\n",
            "\n",
            "--------------------------------------------------\n",
            "Q: What is in front of the Notre Dame Main Building?\n",
            "A: a copper statue of Christ\n",
            "\n",
            "--------------------------------------------------\n",
            "Q: The Basilica of the Sacred heart at Notre Dame is beside to which structure?\n",
            "A: the Main Building\n",
            "\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "questions = dataset['train']['question'][:1000]\n",
        "answers   = dataset['train']['answers'][:1000]\n",
        "qa_pairs  = []\n",
        "for question, answer in zip(questions, answers):\n",
        "    qa_pairs.append(f\"Q: {question}\\nA: {answer['text'][0]}\\n\")\n",
        "\n",
        "# Show the first 3 question-answer pairs\n",
        "for qa in qa_pairs[:3]:\n",
        "    print(qa)\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `beta` will be renamed internally to `bias`. Please use a different name to suppress this warning.\n",
            "A parameter name that contains `gamma` will be renamed internally to `weight`. Please use a different name to suppress this warning.\n",
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "\n",
        "max_length = 100\n",
        "\n",
        "# 初始化 BERT tokenizer 和 vectorizer\n",
        "tokenizer  = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "vectorizer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# 範例句子\n",
        "# sentences = [\n",
        "#     \"Hello, how are you? I am fine\",\n",
        "#     \"BERT is a powerful model.  I am fine\",\n",
        "#     \"Let's use transformers for NLP.  I am fine\"\n",
        "#     \"You are really a muta flicker.  I am fine\"\n",
        "# ]\n",
        "sentences = qa_pairs\n",
        "\n",
        "\n",
        "# Step 1: Tokenize the sentences\n",
        "tokenized_sentences = tokenizer(sentences, padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "\n",
        "# Step 2: Vectorize the sentences\n",
        "input_ids           = tokenized_sentences['input_ids']\n",
        "attention_masks     = tokenized_sentences['attention_mask']\n",
        "with torch.no_grad(): \n",
        "    input_vectors   = vectorizer(input_ids).last_hidden_state * ( attention_masks.unsqueeze(2) )\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def create_dataset(input_vectors, attention_masks):\n",
        "\n",
        "    final_input  = []\n",
        "    final_label  = []\n",
        "    final_mask_1 = []\n",
        "    final_mask_2 = []\n",
        "\n",
        "    for i in range(input_vectors.size(0)): \n",
        "        for j in range(input_vectors.size(1) - 1):\n",
        "            \n",
        "            if attention_masks[i][j + 1] != 0:\n",
        "\n",
        "                factored_mask     = torch.zeros_like(attention_masks[i])\n",
        "                factored_mask[:j] = 1\n",
        "                input             = input_vectors[i] * factored_mask.unsqueeze(1)\n",
        "                final_input.append(input)\n",
        "\n",
        "                label = input_vectors[i][j+1]\n",
        "                final_label.append(label)\n",
        "\n",
        "                mask_2 = factored_mask.unsqueeze(1) * factored_mask.unsqueeze(0)\n",
        "                mask_2 = mask_2.unsqueeze(0)\n",
        "                mask_1 = (mask_2 -1) * 1e20\n",
        "                final_mask_1.append(mask_1)\n",
        "                final_mask_2.append(mask_2)\n",
        "\n",
        "    final_input  = torch.stack(final_input , dim=0)\n",
        "    final_label  = torch.stack(final_label , dim=0)\n",
        "    final_mask_1 = torch.stack(final_mask_1, dim=0)\n",
        "    final_mask_2 = torch.stack(final_mask_2, dim=0)\n",
        "\n",
        "    return final_input, final_label, final_mask_1, final_mask_2\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 生成資料集\n",
        "final_input, final_label, final_mask_1, final_mask_2 = create_dataset(input_vectors, attention_masks)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "dataset    = TensorDataset(final_input, final_label, final_mask_1, final_mask_2)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "sequence_size =  max_length             \n",
        "feature_size = 768          \n",
        "num_layers = 3                      \n",
        "num_heads = 4                \n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'sigmoid'\n",
        "initializer = \"random_normal\"\n",
        "optimizer = 'sgd'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.000001                        \n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = build_model(sequence_size,\n",
        "                    feature_size,\n",
        "                    num_layers,\n",
        "                    num_heads,\n",
        "                    hidden_activation,\n",
        "                    output_activation,\n",
        "                    initializer,\n",
        "                    optimizer,\n",
        "                    loss,\n",
        "                    bias,\n",
        "                    drop_rate,\n",
        "                    alpha)\n",
        "\n",
        "\n",
        "model = model.to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training Progress:  44%|███████████████▊                    | 9003/20522 [01:59<02:36, 73.81batch/s]"
          ]
        }
      ],
      "source": [
        "num_epochs = 100  \n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    model.train()  \n",
        "    \n",
        "    running_loss = 0.0\n",
        "\n",
        "    for batch_idx, (input, label, mask_1, mask_2) in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training Progress\", ncols=100, unit=\"batch\"):\n",
        "\n",
        "        input  = input.to(device)\n",
        "        label  = label.to(device)\n",
        "        mask_1 = mask_1.to(device)\n",
        "        mask_2 = mask_2.to(device)\n",
        "        \n",
        "        optimizer = model.optimizer\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss_function   = model.loss_function\n",
        "        output          = model(input, (mask_1, mask_2))\n",
        "        loss            = loss_function(output, label)\n",
        "        loss.backward()     # get grad\n",
        "\n",
        "        optimizer.step()    # update params\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        \n",
        "\n",
        "    epoch_loss = running_loss / len(dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "prima\n",
            "needs\n",
            "meter\n",
            "meter\n",
            "desk\n",
            "point\n",
            "point\n",
            "progress\n",
            "even\n",
            "meters\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n",
            "multi\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "\n",
        "# 範例句子\n",
        "sentence = \"who are you\"\n",
        "\n",
        "for _ in range(max_length):\n",
        "\n",
        "    # Step 1: Tokenize the sentences\n",
        "    tokenized_sentence = tokenizer(sentence, padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Step 2: Vectorize the sentences\n",
        "    input_id           = tokenized_sentence['input_ids']\n",
        "    attention_mask     = tokenized_sentence['attention_mask']\n",
        "    with torch.no_grad(): \n",
        "        input_vector   = vectorizer(input_id).last_hidden_state \n",
        "\n",
        "    input_vector = input_vector.to(device)\n",
        "    mask_2 = attention_mask[0].unsqueeze(1) * attention_mask[0].unsqueeze(0)\n",
        "    mask_2 = mask_2.unsqueeze(0).unsqueeze(0)\n",
        "    mask_2 = mask_2.to(device)\n",
        "    mask_1 = (mask_2 -1) * 1e20\n",
        "    mask_1 = mask_1.to(device)\n",
        "\n",
        "    model.eval()  \n",
        "    output          = model(input_vector, (mask_1, mask_2))\n",
        "\n",
        "    vocab_embeddings       = vectorizer.get_input_embeddings().weight .to(device)\n",
        "    cos_sim                = F.cosine_similarity(output, vocab_embeddings, dim=1)\n",
        "    most_similar_token_idx = torch.argmax(cos_sim).item()\n",
        "\n",
        "    word                   = tokenizer.convert_ids_to_tokens(most_similar_token_idx)\n",
        "    print(word)\n",
        "\n",
        "    \n",
        "    \n",
        "    # vocab = tokenizer.get_vocab()\n",
        "    # while most_similar_token_idx in vocab and vocab[most_similar_token_idx].startswith('[unused'):\n",
        "    #     most_similar_token_idx = torch.argmax(cos_sim).item()  # Recompute the most similar token\n",
        "\n",
        "    # from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "    # tokenizer_ = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "    # model_     = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    # output     = model_.generate(torch.tensor([[most_similar_token_idx]]), max_length=20)\n",
        "    # word       = tokenizer_.decode(output[0], skip_special_tokens=True)\n",
        "    # print(word)\n",
        "\n",
        "    sentence += '' + word\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
