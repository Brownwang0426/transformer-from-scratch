{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "(for local) \\\n",
        "CUDA Toolkit 11.8 \\\n",
        "cuDNN 8.9.x \\\n",
        "pip install torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cu118  \n",
        "其餘套件自己想辦法"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# Importing modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import display, clear_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "from model import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# Checking cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Aj5V_vlwSxd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device 0: NVIDIA GeForce RTX 4090\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "source = \"squad\" # squad  natural_questions\n",
        "\n",
        "retrain = True\n",
        "\n",
        "interval   = 100\n",
        "train_size = 2000\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "max_length = 200\n",
        "\n",
        "sequence_size =  max_length             \n",
        "feature_size = 768          \n",
        "num_layers = 3                      \n",
        "num_heads = 4                \n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'tanh'\n",
        "initializer = \"xavier_normal\"\n",
        "optimizer = 'adam'\n",
        "loss = 'mean_squared_error'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.000001       \n",
        "\n",
        "num_epochs = 100  \n",
        "batch_size = 1\n",
        "\n",
        "model_directory = f'model.pth'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Build model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Initialize the model\n",
        "model = build_model(sequence_size,\n",
        "                    feature_size,\n",
        "                    num_layers,\n",
        "                    num_heads,\n",
        "                    hidden_activation,\n",
        "                    output_activation,\n",
        "                    initializer,\n",
        "                    optimizer,\n",
        "                    loss,\n",
        "                    bias,\n",
        "                    drop_rate,\n",
        "                    alpha)\n",
        "\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "if retrain:\n",
        "    model_dict = torch.load(model_directory)\n",
        "    model.load_state_dict(model_dict[f'model'])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparing data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the IMDB dataset\n",
        "dataset     = load_dataset(source, trust_remote_code=True )\n",
        "questions   = dataset['train']['question']\n",
        "answers     = dataset['train']['answers' ]\n",
        "\n",
        "# Ensure you don't pick more samples than available\n",
        "train_size  = min(train_size, len(questions))\n",
        "\n",
        "# 開始抽取資料\n",
        "for _ in range(interval):\n",
        "\n",
        "    # Randomly select indices\n",
        "    random_indices = random.sample(range(len(questions)), train_size)\n",
        "\n",
        "    # Get the selected subset of questions and answers\n",
        "    questions = [questions[i] for i in random_indices]\n",
        "    answers   = [answers[i]   for i in random_indices]\n",
        "\n",
        "    # Create QA pairs\n",
        "    qa_pairs  = []\n",
        "    for question, answer in zip(questions, answers):\n",
        "        qa_pairs.append(f\"[CLS] {question} [SEP] {answer['text'][0]} [SEP] \")\n",
        "\n",
        "    # Show the first 3 question-answer pairs\n",
        "    for qa in qa_pairs[:3]:\n",
        "        print(qa)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # 初始化 BERT tokenizer 和 vectorizer\n",
        "    tokenizer  = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    vectorizer = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "    # 範例句子\n",
        "    # sentences = [\n",
        "    #     \"Hello, how are you? I am fine\",\n",
        "    #     \"BERT is a powerful model.  I am fine\",\n",
        "    #     \"Let's use transformers for NLP.  I am fine\"\n",
        "    #     \"You are really a muta flicker.  I am fine\"\n",
        "    # ]\n",
        "    sentences = qa_pairs\n",
        "\n",
        "    # Step 1: Tokenize the sentences\n",
        "    tokenized_sentences = tokenizer(sentences, add_special_tokens=False, padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # Step 2: Vectorize the sentences\n",
        "    input_ids           = tokenized_sentences['input_ids']\n",
        "    attention_masks     = tokenized_sentences['attention_mask']\n",
        "    with torch.no_grad(): \n",
        "        input_vectors   = vectorizer(input_ids).last_hidden_state * ( attention_masks.unsqueeze(2) )\n",
        "\n",
        "    def create_dataset(input_vectors, attention_masks):\n",
        "\n",
        "        final_input  = []\n",
        "        final_label  = []\n",
        "        final_mask_1 = []\n",
        "        final_mask_2 = []\n",
        "\n",
        "        for i in range(input_vectors.size(0)): \n",
        "            for j in range(input_vectors.size(1) - 1):\n",
        "                \n",
        "                if attention_masks[i][j + 1] != 0:\n",
        "\n",
        "                    factored_mask       = torch.zeros_like(attention_masks[i])\n",
        "                    factored_mask[:j+1] = 1\n",
        "                    input               = input_vectors[i] * factored_mask.unsqueeze(1)\n",
        "                    final_input.append(input)\n",
        "\n",
        "                    label = input_vectors[i][j+1]\n",
        "                    final_label.append(label)\n",
        "\n",
        "                    mask_2 = factored_mask.unsqueeze(1) * factored_mask.unsqueeze(0)\n",
        "                    mask_2 = mask_2.unsqueeze(0)\n",
        "                    mask_1 = (mask_2 -1) * 1e20\n",
        "                    final_mask_1.append(mask_1)\n",
        "                    final_mask_2.append(mask_2)\n",
        "\n",
        "        final_input  = torch.stack(final_input , dim=0)\n",
        "        final_label  = torch.stack(final_label , dim=0)\n",
        "        final_mask_1 = torch.stack(final_mask_1, dim=0)\n",
        "        final_mask_2 = torch.stack(final_mask_2, dim=0)\n",
        "\n",
        "        return final_input, final_label, final_mask_1, final_mask_2\n",
        "\n",
        "    # 生成資料集\n",
        "    final_input, final_label, final_mask_1, final_mask_2 = create_dataset(input_vectors, attention_masks)\n",
        "\n",
        "    # 彙整資料\n",
        "    dataset    = TensorDataset(final_input, final_label, final_mask_1, final_mask_2)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "    # 訓練模型\n",
        "    for epoch in range(num_epochs):\n",
        "        \n",
        "        model.train()  \n",
        "        \n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (input, label, mask_1, mask_2) in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training Progress\", ncols=100, unit=\"batch\"):\n",
        "\n",
        "            input  = input.to(device)\n",
        "            label  = label.to(device)\n",
        "            mask_1 = mask_1.to(device)\n",
        "            mask_2 = mask_2.to(device)\n",
        "            \n",
        "            optimizer = model.optimizer\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_function   = model.loss_function\n",
        "            output          = model(input, (mask_1, mask_2))\n",
        "            loss            = loss_function(output, label)\n",
        "            loss.backward()     # get grad\n",
        "\n",
        "            optimizer.step()    # update params\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        model_dict = {}\n",
        "        model_dict[f'model'] = model.state_dict()\n",
        "        torch.save(model_dict, model_directory)\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model_dict = {}\n",
        "model_dict[f'model'] = model.state_dict()\n",
        "torch.save(model_dict, model_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKTJbMhmZvVI"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "llm",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
