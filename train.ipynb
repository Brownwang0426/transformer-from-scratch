{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Brownwang0426/transformer-from-scratch/blob/main/train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PhgpT8hhhhLd"
      },
      "source": [
        "# 手刻簡單的文字生成 transformer\n",
        "\n",
        "示範如何純粹用 pytorch 手刻一個簡單的文字生成 transformer \\\n",
        "並強化自己對於 transformer 的理解以及操作能力 \\\n",
        "如果有機會，未來會再增加 numpy 版本，並且使用 numpy 手刻 error back-propagation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZQm4saPhUYV"
      },
      "source": [
        "# For colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVZs5loNPbPn",
        "outputId": "2a6728c6-9f36-4767-ac00-7d3ebf504803"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformer-from-scratch' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/Brownwang0426/transformer-from-scratch.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hj-G_J3dPbPo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.chdir('/content/transformer-from-scratch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdyb1d_hcml8",
        "outputId": "449c000d-fc39-412a-f726-1beb60e4e124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu121)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (0.3.8)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: IPython in /usr/local/lib/python3.11/dist-packages (7.34.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.9.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.11/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.11/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.11/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.85)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.11)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.27.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.11/dist-packages (from IPython) (75.1.0)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.11/dist-packages (from IPython) (0.19.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from IPython) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.11/dist-packages (from IPython) (0.7.5)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.11/dist-packages (from IPython) (5.7.1)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from IPython) (3.0.50)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.11/dist-packages (from IPython) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.11/dist-packages (from IPython) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.11/dist-packages (from IPython) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.11/dist-packages (from IPython) (4.9.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (24.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.11/dist-packages (from jedi>=0.16->IPython) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.11/dist-packages (from pexpect>4.3->IPython) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython) (0.2.13)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2024.12.14)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch dill datasets tqdm numpy IPython"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZCN3-VOhY-M"
      },
      "source": [
        "# For local\n",
        "CUDA Toolkit 11.8 \\\n",
        "cuDNN 8.9.x \\\n",
        "pip install torch==2.0.1 --extra-index-url https://download.pytorch.org/whl/cu118  \n",
        "其餘套件可自行下載"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6kEiLW6f-v2"
      },
      "source": [
        "# 導入官方套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mVWhBy17f-v3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.utils.rnn as rnn_utils\n",
        "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
        "\n",
        "import csv\n",
        "\n",
        "import multiprocessing as mp\n",
        "import os\n",
        "import sys\n",
        "import copy\n",
        "import random\n",
        "import gc\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "\n",
        "import itertools\n",
        "\n",
        "import dill\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "import torch\n",
        "from transformers import BertTokenizer, BertModel, AutoTokenizer, AutoModel\n",
        "import numpy as np\n",
        "\n",
        "from IPython.display import display, clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT-dGum0chjy"
      },
      "source": [
        "# 導入客製化套件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MdLUtG5nchj0"
      },
      "outputs": [],
      "source": [
        "from model import *"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElhExcVoSxd7"
      },
      "source": [
        "# 確認有無讀取到 cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aj5V_vlwSxd8",
        "outputId": "59fe4ad5-6dd3-4c6c-d2aa-20071b2bdf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device 0: Tesla T4\n",
            "using cuda...\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    for i in range(torch.cuda.device_count()):\n",
        "        print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
        "    device_index = 0\n",
        "    device = torch.device(f\"cuda:{device_index}\")\n",
        "    print('using cuda...')\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print('using cpu...')\n",
        "\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.backends.cudnn.benchmark = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ACgMI7chj6"
      },
      "source": [
        "# 參數區域"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Q7t3r9iQVLrn"
      },
      "outputs": [],
      "source": [
        "# 其他可以用的有 squad  natural_questions\n",
        "source = \"daily_dialog\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KmtYCrXKVLrn"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 初始化 BERT tokenizer 和 vectorizer\n",
        "tokenizer  = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "vectorizer = AutoModel.from_pretrained(\"bert-base-uncased\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AcDtqnZ-chj7"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 要進行幾回合\n",
        "interval   = 1000000\n",
        "\n",
        "# 每一回合要抽取多少樣本\n",
        "train_size = 50\n",
        "\n",
        "# 每個樣本要看多長的字\n",
        "max_length = 200\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "JIWY3GRqchj8"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 是否讀取之前訓練的模型\n",
        "retrain = True\n",
        "\n",
        "# 模型相關參數\n",
        "sequence_size = max_length\n",
        "feature_size = vectorizer.config.hidden_size\n",
        "output_size = tokenizer.vocab_size\n",
        "num_layers = 6\n",
        "num_heads = 4\n",
        "hidden_activation = 'tanh'\n",
        "output_activation = 'softmax'\n",
        "initializer = \"xavier_normal\"\n",
        "optimizer = 'adam'\n",
        "loss = 'nl_loss'\n",
        "bias = False\n",
        "drop_rate = 0.0\n",
        "alpha = 0.00001\n",
        "\n",
        "# 訓練相關參數\n",
        "num_epochs = 1\n",
        "batch_size = 1\n",
        "\n",
        "# 存檔區域\n",
        "model_directory = f'model.pth'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WxQLkClpepJM"
      },
      "outputs": [],
      "source": [
        "# 機器回覆你的時候，要用多少個字（當沒有結束標記出現的時候）\n",
        "response_length = 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-C7Lo0AWchj8"
      },
      "source": [
        "# 建立模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFBrvqtIchj9",
        "outputId": "ef5b91d3-a1c2-4a3f-ba9e-367a12efbafc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model not loaded. Now using new model.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# 建立模型\n",
        "model = build_model(sequence_size,\n",
        "                    feature_size,\n",
        "                    output_size,\n",
        "                    num_layers,\n",
        "                    num_heads,\n",
        "                    hidden_activation,\n",
        "                    output_activation,\n",
        "                    initializer,\n",
        "                    optimizer,\n",
        "                    loss,\n",
        "                    bias,\n",
        "                    drop_rate,\n",
        "                    alpha)\n",
        "\n",
        "# 將模型放到 cuda\n",
        "model = model.to(device)\n",
        "\n",
        "# 讀取參數\n",
        "if retrain:\n",
        "    try:\n",
        "        model_dict = torch.load(model_directory)\n",
        "        model.load_state_dict(model_dict[f'model'])\n",
        "        print('Model loaded.')\n",
        "    except:\n",
        "        print('Model not loaded. Now using new model.')\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDu9hgebchj9"
      },
      "source": [
        "# 準備資料並訓練模型"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Pjoc3YSYchj-"
      },
      "outputs": [],
      "source": [
        "def create_dataset(input_vectors, input_ids, output_size, attention_masks):\n",
        "\n",
        "    final_input  = []\n",
        "    final_label  = []\n",
        "    final_mask_1 = []\n",
        "    final_mask_2 = []\n",
        "\n",
        "    for i in tqdm(range(input_vectors.size(0))):\n",
        "        for j in range(input_vectors.size(1) - 1):\n",
        "\n",
        "            if attention_masks[i][j + 1] != 0:\n",
        "\n",
        "                factored_mask       = torch.zeros_like(attention_masks[i])\n",
        "                factored_mask[:j+1] = 1\n",
        "                input               = input_vectors[i] * factored_mask.unsqueeze(1)\n",
        "                final_input.append(input)\n",
        "\n",
        "                label = input_ids[i][j + 1]\n",
        "                final_label.append(label)\n",
        "\n",
        "                mask_2 = factored_mask.unsqueeze(1) * factored_mask.unsqueeze(0)\n",
        "                mask_2 = mask_2.unsqueeze(0)\n",
        "                mask_1 = (mask_2 -1) * 1e20\n",
        "                final_mask_1.append(mask_1)\n",
        "                final_mask_2.append(mask_2)\n",
        "\n",
        "    final_input  = torch.stack(final_input , dim=0)\n",
        "    final_label  = torch.tensor(final_label, dtype=torch.long)\n",
        "    final_mask_1 = torch.stack(final_mask_1, dim=0)\n",
        "    final_mask_2 = torch.stack(final_mask_2, dim=0)\n",
        "\n",
        "    return final_input, final_label, final_mask_1, final_mask_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "W_Kal2yxVLrp"
      },
      "outputs": [],
      "source": [
        "# 讀取資料\n",
        "dataset     = load_dataset(source, trust_remote_code=True )\n",
        "\n",
        "questions = []\n",
        "answers   = []\n",
        "# Process each dialogue\n",
        "for dialog in dataset['train']['dialog']:\n",
        "    # Separate utterances into Person A and Person B\n",
        "    person_a_utterances = dialog[::2]   # Odd-indexed utterances are Person A's\n",
        "    person_b_utterances = dialog[1::2]  # Even-indexed utterances are Person B's\n",
        "\n",
        "    person_max_length = max(len(person_a_utterances), len(person_b_utterances))\n",
        "    if len(person_a_utterances) < person_max_length:\n",
        "        person_b_utterances = person_b_utterances[:-1]\n",
        "    if len(person_b_utterances) < person_max_length:\n",
        "        person_a_utterances = person_a_utterances[:-1]\n",
        "\n",
        "    # Join Person A's and Person B's utterances into strings\n",
        "    questions.extend(person_a_utterances)\n",
        "    answers.extend(person_b_utterances)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0chhWYKWchj-",
        "outputId": "f4c861a9-fc37-42e0-a881-242b2abe63e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[CLS]  since everyone is different , your personal trainer will help you find out all the exercise equipments that are suitable for you fitness level . Then you'llbe taught all the necessary techniques to us  [SEP]  sounds pretty good . How much do you charge ?  [SEP] \n",
            "--------------------------------------------------\n",
            "[CLS]  We'd prefer separate checks .  [SEP]  OK , one with the apple pie pays $ 18 and the other $ 12 .  [SEP] \n",
            "--------------------------------------------------\n",
            "[CLS]  That chair over there , but please be careful with it . It was a gift from my mother-in-law .  [SEP]  Don't worry , I won't drop it . Wow , it's really heavy . I don't think I can move it by myself .  [SEP] \n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:01<00:00, 34.35it/s]\n",
            "Training Progress: 100%|█████████████████████████████████████| 1575/1575 [00:41<00:00, 38.05batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1] finished. Average Loss: 7.7073\n",
            "[CLS]  No , no ... It was just the wrong size .  [SEP]  Would you be interested in an exchange as opposed to a refund ? I think I can help you to find the appropriate size .  [SEP] \n",
            "--------------------------------------------------\n",
            "[CLS] Hey man , what do you have on tap ?  [SEP]  Heineken and Budweiser . We have a two-for-one happy hour special .  [SEP] \n",
            "--------------------------------------------------\n",
            "[CLS]  How's Mrs Zu doing these days ?  [SEP]  She's fine , thanks . Actually , she's the reason I'm here . It's our Wedding Anniversary in 2 weeks and I want to get her something special . She's been nagging me about our furniture , you see .  [SEP] \n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 50/50 [00:02<00:00, 24.86it/s]\n",
            "Training Progress: 100%|█████████████████████████████████████| 1619/1619 [00:43<00:00, 37.26batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1] finished. Average Loss: 6.2516\n",
            "[CLS] Let's go see pandas ! The news said that two pandas arrived at the zoo last week .  [SEP]  Great ! I would like to see cute pandas , too .  [SEP] \n",
            "--------------------------------------------------\n",
            "[CLS]  In that case , here's my VISA .  [SEP]  Thank you . Your room number is 507 , queen bed , nonsmoking . Is that agreeable to you , sir ?  [SEP] \n",
            "--------------------------------------------------\n",
            "[CLS] I swear I will never shop at a street market in China . It's a terrible place full of excellent profiteers ! You have to keep an eye whenever it comes to paying for something .  [SEP]  cool down , Harry ! What are you really mad about ? Did you get ripped off ?  [SEP] \n",
            "--------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 24%|██▍       | 12/50 [00:00<00:01, 29.41it/s]"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# 確定要抽取的 QA 大小\n",
        "train_size  = min(train_size, len(questions))\n",
        "\n",
        "# 開始抽取資料\n",
        "for _ in range(interval):\n",
        "\n",
        "    # 隨機 indices\n",
        "    random_indices = random.sample(range(len(questions)), train_size)\n",
        "\n",
        "    # 抽取 QA\n",
        "    q_samples = [questions[i] for i in random_indices]\n",
        "    a_samples = [answers[i]   for i in random_indices]\n",
        "\n",
        "    # 建立 QA\n",
        "    qa_pairs  = []\n",
        "    for q_sample, a_sample in zip(q_samples, a_samples):\n",
        "        qa_pairs.append(f\"[CLS] {q_sample} [SEP] {a_sample} [SEP] \")\n",
        "\n",
        "    # 偷看一下 QA 裡面有什麼東西\n",
        "    for qa in qa_pairs[:3]:\n",
        "        print(qa)\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "    # 改一下名稱\n",
        "    sentences = qa_pairs\n",
        "\n",
        "    # 將 QA tokenize\n",
        "    tokenized_sentences = tokenizer(sentences, add_special_tokens=False, padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # 將 QA vectorize\n",
        "    input_ids           = tokenized_sentences['input_ids']\n",
        "    attention_masks     = tokenized_sentences['attention_mask']\n",
        "    with torch.no_grad():\n",
        "        input_vectors   = vectorizer(input_ids).last_hidden_state * ( attention_masks.unsqueeze(2) )\n",
        "\n",
        "    # 生成 QA 訓練集\n",
        "    final_input, final_label, final_mask_1, final_mask_2 = create_dataset(input_vectors, input_ids, output_size, attention_masks)\n",
        "\n",
        "    # 彙整 QA 訓練集\n",
        "    dataset    = TensorDataset(final_input, final_label, final_mask_1, final_mask_2)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # 訓練模型\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        running_loss = 0.0\n",
        "\n",
        "        for batch_idx, (input, label, mask_1, mask_2) in tqdm(enumerate(dataloader), total=len(dataloader), desc=\"Training Progress\", ncols=100, unit=\"batch\"):\n",
        "\n",
        "            input  = input.to(device)\n",
        "            label  = label.to(device)\n",
        "            mask_1 = mask_1.to(device)\n",
        "            mask_2 = mask_2.to(device)\n",
        "\n",
        "            optimizer = model.optimizer\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            loss_function   = model.loss_function\n",
        "            output          = model(input, (mask_1, mask_2))\n",
        "            loss            = loss_function(output, label)\n",
        "            loss.backward()     # get grad\n",
        "\n",
        "            optimizer.step()    # update params\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] finished. Average Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "        model_dict = {}\n",
        "        model_dict[f'model'] = model.state_dict()\n",
        "        torch.save(model_dict, model_directory)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TpHHO3UevbY"
      },
      "source": [
        "# 來跟這個小模型用英文聊聊天吧"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M85l53w3chkD"
      },
      "outputs": [],
      "source": [
        "# 你要問的句子\n",
        "sentence = \"Please talk like human, dummy!\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L6PUPm6AchkD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# 幫你的問題補上 [CSL] 以及 [SEP] ，讓機器可以知道問句的開始與結束\n",
        "sentence = \"[CLS] \" + sentence + \" [SEP] \"\n",
        "\n",
        "# 機器的回答\n",
        "response = ''\n",
        "\n",
        "# 開始遞迴\n",
        "for i in range(response_length):\n",
        "\n",
        "    # 將 QA tokenize\n",
        "    tokenized_sentence = tokenizer(sentence, add_special_tokens=False, padding='max_length', max_length=max_length, truncation=True, return_tensors=\"pt\")\n",
        "\n",
        "    # 將 QA vectorize\n",
        "    input_id           = tokenized_sentence['input_ids']\n",
        "    attention_mask     = tokenized_sentence['attention_mask']\n",
        "    with torch.no_grad():\n",
        "        input_vector   = vectorizer(input_id).last_hidden_state * ( attention_mask.unsqueeze(2) )\n",
        "    input_vector = input_vector.to(device)\n",
        "\n",
        "    # 製作對應 QA 長度的 mask\n",
        "    mask_2 = attention_mask[0].unsqueeze(1) * attention_mask[0].unsqueeze(0)\n",
        "    mask_2 = mask_2.unsqueeze(0).unsqueeze(0)\n",
        "    mask_2 = mask_2.to(device)\n",
        "    mask_1 = (mask_2 -1) * 1e20\n",
        "    mask_1 = mask_1.to(device)\n",
        "\n",
        "    # 將你的問題向量丟到模型去吧\n",
        "    model.eval()\n",
        "    output                  = model(input_vector, (mask_1, mask_2))\n",
        "\n",
        "    # 選擇最高概率的詞\n",
        "    most_probable_token_idx = torch.argmax(output, dim=-1).item()\n",
        "    word = tokenizer.convert_ids_to_tokens(most_probable_token_idx)\n",
        "\n",
        "    # 將機器人吐出的那個字拼接回去\n",
        "    if word not in ['[SEP]']:\n",
        "        sentence += ' ' + word\n",
        "        response += ' ' + word\n",
        "        clear_output(wait=True)  # Clear the previous output\n",
        "        print(response, flush=False)\n",
        "        display()  # Display the updated output\n",
        "    else:\n",
        "        print(\"[END]\")\n",
        "        break\n",
        "\n",
        "    # 將機器人吐出的那個字拼接回去\n",
        "    # while word in ['[SEP]', '.', ',', '?']:\n",
        "    #     output[0][most_probable_token_idx] = 0\n",
        "    #     most_probable_token_idx = torch.argmax(output, dim=-1).item()\n",
        "    #     word = tokenizer.convert_ids_to_tokens(most_probable_token_idx)\n",
        "    # sentence += ' ' + word\n",
        "    # response += ' ' + word\n",
        "    # clear_output(wait=True)  # Clear the previous output\n",
        "    # print(response, flush=False)\n",
        "    # display()  # Display the updated output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0krnWZyBchkE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3fAercPIchkE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3iwR5cGQVLrq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Genrl",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}